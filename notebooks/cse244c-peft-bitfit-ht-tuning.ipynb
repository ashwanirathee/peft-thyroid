{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2733529,"sourceType":"datasetVersion","datasetId":1634980},{"sourceId":8669991,"sourceType":"datasetVersion","datasetId":5195844},{"sourceId":12054481,"sourceType":"datasetVersion","datasetId":7577241}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install the requirments for ENV setup","metadata":{}},{"cell_type":"code","source":"!pip install -Uqqq pip --progress-bar off\n!pip install -qqq torch==2.1 --progress-bar off\n!pip install -qqq transformers==4.34.1 --progress-bar off\n!pip install -qqq accelerate==0.23.0 --progress-bar off\n!pip install -qqq bitsandbytes==0.41.1 --progress-bar off\n!pip install -qqq llava-torch==1.1.1 --progress-bar off\n# !pip install gdown","metadata":{"execution":{"iopub.status.busy":"2025-06-05T01:01:37.652727Z","iopub.execute_input":"2025-06-05T01:01:37.653021Z","iopub.status.idle":"2025-06-05T01:06:34.744366Z","shell.execute_reply.started":"2025-06-05T01:01:37.652979Z","shell.execute_reply":"2025-06-05T01:06:34.743256Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 2.19.2 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.2.1 requires httpx>=0.25.0, but you have httpx 0.24.0 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkaggle-environments 1.14.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.10.22 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# load the lib and modules required for test run of LLMs","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:40:05.813101Z","iopub.execute_input":"2023-11-11T08:40:05.813542Z","iopub.status.idle":"2023-11-11T08:40:05.818897Z","shell.execute_reply.started":"2023-11-11T08:40:05.813506Z","shell.execute_reply":"2023-11-11T08:40:05.817802Z"}}},{"cell_type":"code","source":"import textwrap\nfrom io import BytesIO\n\nimport requests\nimport torch\nfrom llava.constants import DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\nfrom llava.conversation import SeparatorStyle, conv_templates\nfrom llava.mm_utils import (\n    KeywordsStoppingCriteria,\n    get_model_name_from_path,\n    process_images,\n    tokenizer_image_token,\n)\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:06:34.746369Z","iopub.execute_input":"2025-06-05T01:06:34.746685Z","iopub.status.idle":"2025-06-05T01:06:50.852215Z","shell.execute_reply.started":"2025-06-05T01:06:34.746659Z","shell.execute_reply":"2025-06-05T01:06:50.851350Z"}},"outputs":[{"name":"stdout","text":"[2025-06-05 01:06:39,530] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"2025-06-05 01:06:42.778540: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-06-05 01:06:42.778694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-05 01:06:42.909093: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"disable_torch_init()\nMODEL = \"microsoft/llava-med-v1.5-mistral-7b\"\nmodel_name = get_model_name_from_path(MODEL)\nmodel_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:06:50.853239Z","iopub.execute_input":"2025-06-05T01:06:50.853472Z","iopub.status.idle":"2025-06-05T01:06:50.859269Z","shell.execute_reply.started":"2025-06-05T01:06:50.853453Z","shell.execute_reply":"2025-06-05T01:06:50.858588Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'llava-med-v1.5-mistral-7b'"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Load the test images for testing the model","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:40:24.801991Z","iopub.execute_input":"2023-11-11T08:40:24.802597Z","iopub.status.idle":"2023-11-11T08:40:24.806589Z","shell.execute_reply.started":"2023-11-11T08:40:24.802563Z","shell.execute_reply":"2023-11-11T08:40:24.805566Z"}}},{"cell_type":"code","source":"tokenizer, model, image_processor, context_len = load_pretrained_model(\n                                                    model_path=MODEL, \n                                                    model_base=None, \n                                                    model_name=model_name, \n                                                    load_8bit=True )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:06:50.860299Z","iopub.execute_input":"2025-06-05T01:06:50.860553Z","iopub.status.idle":"2025-06-05T01:08:54.580204Z","shell.execute_reply.started":"2025-06-05T01:06:50.860533Z","shell.execute_reply":"2025-06-05T01:08:54.579469Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455b4d52814d4f9881817e71245630ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67293683be10468f82ce9a90fd48ba37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5d7a075e2d4148ae268d3e8d9cd434"}},"metadata":{}},{"name":"stderr","text":"You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c46e0fa3d7124959a78ce4bfaa2ecf53"}},"metadata":{}},{"name":"stderr","text":"You are using a model of type llava_mistral to instantiate a model of type llava. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)fetensors.index.json:   0%|          | 0.00/73.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"780075a235ca447db08907599a07182e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03547527f2a346e49e7d2a75ff575a88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00004.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e669eca2cd3b48d58da4652be619c33e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25811dcb9a24934b2bf40afb9b10c25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f690bafd02f4df7af75c8d206ae4b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00004.safetensors:   0%|          | 0.00/262M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f2728be4c5942d8a268677fb216f704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e86e37e725b44ca09965ea6cc5c207e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df6a7e45b04d44d584309aad76af1e5b"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at microsoft/llava-med-v1.5-mistral-7b and are newly initialized: ['model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4972b69e1c741ea9968b2dc5064bd3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46a971a37b3a46de9ceb29118c7f6101"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4be5d18829b4f4aab6106e96cd43d25"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def load_image(image_file):\n    if image_file.startswith(\"http://\") or image_file.startswith(\"https://\"):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    else:\n        image = Image.open(image_file).convert(\"RGB\")\n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:08:54.582891Z","iopub.execute_input":"2025-06-05T01:08:54.583564Z","iopub.status.idle":"2025-06-05T01:08:54.589015Z","shell.execute_reply.started":"2025-06-05T01:08:54.583529Z","shell.execute_reply":"2025-06-05T01:08:54.588240Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def process_image(image):\n    args = {\"image_aspect_ratio\": \"pad\"}\n    image_tensor = process_images([image], image_processor, args)\n    return image_tensor.to(model.device, dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:08:54.589973Z","iopub.execute_input":"2025-06-05T01:08:54.590188Z","iopub.status.idle":"2025-06-05T01:08:54.602133Z","shell.execute_reply.started":"2025-06-05T01:08:54.590170Z","shell.execute_reply":"2025-06-05T01:08:54.601273Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_prompt(prompt: str):\n    CONV_MODE = 'llava_v0'\n    conv = conv_templates[CONV_MODE].copy()\n    roles = conv.roles\n    prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n    conv.append_message(roles[0], prompt)\n    conv.append_message(roles[1], None)\n    return conv.get_prompt(), conv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:08:54.603075Z","iopub.execute_input":"2025-06-05T01:08:54.603303Z","iopub.status.idle":"2025-06-05T01:08:54.613336Z","shell.execute_reply.started":"2025-06-05T01:08:54.603284Z","shell.execute_reply":"2025-06-05T01:08:54.612491Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def ask_image(image: Image, prompt: str):\n    image_tensor = process_image(image)\n    prompt, conv = create_prompt(prompt)\n    input_ids = (\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n        .unsqueeze(0)\n        .to(model.device)\n    )\n\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n    stopping_criteria = KeywordsStoppingCriteria(\n        keywords=[stop_str], tokenizer=tokenizer, input_ids=input_ids\n    )\n\n    with torch.inference_mode():\n        output_ids = model.generate(\n            input_ids,\n            images=image_tensor,\n            do_sample=True,\n            temperature=0.01,\n            max_new_tokens=512,\n            use_cache=True,\n            stopping_criteria=[stopping_criteria],\n        )\n    return tokenizer.decode(\n        output_ids[0, input_ids.shape[1] :], skip_special_tokens=True\n    ).strip()","metadata":{"execution":{"iopub.status.busy":"2025-06-05T01:08:54.614291Z","iopub.execute_input":"2025-06-05T01:08:54.614544Z","iopub.status.idle":"2025-06-05T01:08:54.628691Z","shell.execute_reply.started":"2025-06-05T01:08:54.614525Z","shell.execute_reply":"2025-06-05T01:08:54.627889Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Load the saved dataset\ndata = torch.load('/kaggle/input/peft-thyroid-dataset/augmented_dataset.pt', weights_only=False)\n\n# Print sizes\nprint(\"Train images:\", data['train_images'].shape)\nprint(\"Train labels:\", len(data['train_labels']))\nprint(\"Train IDs:\", len(data['train_ids']))  # list of strings\n\nprint(\"Val images:\", data['val_images'].shape)\nprint(\"Val labels:\", len(data['val_labels']))\nprint(\"Val IDs:\", len(data['val_ids']))\n\nprint(\"Test images:\", data['test_images'].shape)\nprint(\"Test labels:\", len(data['test_labels']))\nprint(\"Test IDs:\", len(data['test_ids']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:08:54.629661Z","iopub.execute_input":"2025-06-05T01:08:54.629849Z","iopub.status.idle":"2025-06-05T01:09:07.919713Z","shell.execute_reply.started":"2025-06-05T01:08:54.629833Z","shell.execute_reply":"2025-06-05T01:09:07.918866Z"}},"outputs":[{"name":"stdout","text":"Train images: torch.Size([1035, 3, 360, 560])\nTrain labels: 1035\nTrain IDs: 1035\nVal images: (45, 3, 360, 560)\nVal labels: 45\nVal IDs: 45\nTest images: (45, 3, 360, 560)\nTest labels: 45\nTest IDs: 45\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\ndef map_label_to_tirads(label):\n    if \"2\" in label: return \"TI-RADS 2\"\n    elif \"3\" in label: return \"TI-RADS 3\"\n    elif \"4\" in label: return \"TI-RADS 4\"\n    elif \"5\" in label: return \"TI-RADS 5\"\n    else: return \"Unknown\"\n\nclass ThyroidDataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = images\n\n        label_map = {\"TI-RADS 2\": 0, \"TI-RADS 3\": 1, \"TI-RADS 4\": 2, \"TI-RADS 5\": 3}\n        self.labels = []\n\n        for label in labels:\n            if not isinstance(label, str):\n                raise ValueError(f\"Unexpected non-string label: {label}\")\n            coarse_label = map_label_to_tirads(label)\n            if coarse_label not in label_map:\n                raise ValueError(f\"🚨 Unknown label: {label} → {coarse_label}\")\n            self.labels.append(label_map[coarse_label])\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'pixel_values': self.images[idx],  # tensor\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n\n# Create Datasets\ntrain_dataset = ThyroidDataset(data['train_images'], data['train_labels'])\nval_dataset = ThyroidDataset(data['val_images'], data['val_labels'])\n\n# Dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:09:07.920928Z","iopub.execute_input":"2025-06-05T01:09:07.921189Z","iopub.status.idle":"2025-06-05T01:09:07.971639Z","shell.execute_reply.started":"2025-06-05T01:09:07.921168Z","shell.execute_reply":"2025-06-05T01:09:07.970799Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torch import nn\ndevice = \"cuda\"\n\n# Only add the classifier if it doesn't exist\nif not hasattr(model, \"classifier\"):\n    model.classifier = nn.Linear(model.config.hidden_size, 4).to(device)\n    nn.init.xavier_uniform_(model.classifier.weight)\n    nn.init.zeros_(model.classifier.bias)\n\n# Make sure classifier parameters are trainable\nfor param in model.classifier.parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:09:07.972597Z","iopub.execute_input":"2025-06-05T01:09:07.972889Z","iopub.status.idle":"2025-06-05T01:09:11.783774Z","shell.execute_reply.started":"2025-06-05T01:09:07.972858Z","shell.execute_reply":"2025-06-05T01:09:11.782959Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Set BitFit — biases and classifier only\nfor name, param in model.named_parameters():\n    if \".bias\" in name or \"classifier\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:09:11.784751Z","iopub.execute_input":"2025-06-05T01:09:11.785078Z","iopub.status.idle":"2025-06-05T01:09:11.792116Z","shell.execute_reply.started":"2025-06-05T01:09:11.785055Z","shell.execute_reply":"2025-06-05T01:09:11.791321Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    if param.dtype == torch.float16:\n        print(f\"Converting {name} from float16 to float32\")\n        param.data = param.data.float()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:09:11.793075Z","iopub.execute_input":"2025-06-05T01:09:11.793334Z","iopub.status.idle":"2025-06-05T01:09:11.831924Z","shell.execute_reply.started":"2025-06-05T01:09:11.793315Z","shell.execute_reply":"2025-06-05T01:09:11.831163Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Converting model.embed_tokens.weight from float16 to float32\nConverting model.layers.0.input_layernorm.weight from float16 to float32\nConverting model.layers.0.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.1.input_layernorm.weight from float16 to float32\nConverting model.layers.1.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.2.input_layernorm.weight from float16 to float32\nConverting model.layers.2.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.3.input_layernorm.weight from float16 to float32\nConverting model.layers.3.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.4.input_layernorm.weight from float16 to float32\nConverting model.layers.4.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.5.input_layernorm.weight from float16 to float32\nConverting model.layers.5.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.6.input_layernorm.weight from float16 to float32\nConverting model.layers.6.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.7.input_layernorm.weight from float16 to float32\nConverting model.layers.7.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.8.input_layernorm.weight from float16 to float32\nConverting model.layers.8.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.9.input_layernorm.weight from float16 to float32\nConverting model.layers.9.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.10.input_layernorm.weight from float16 to float32\nConverting model.layers.10.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.11.input_layernorm.weight from float16 to float32\nConverting model.layers.11.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.12.input_layernorm.weight from float16 to float32\nConverting model.layers.12.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.13.input_layernorm.weight from float16 to float32\nConverting model.layers.13.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.14.input_layernorm.weight from float16 to float32\nConverting model.layers.14.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.15.input_layernorm.weight from float16 to float32\nConverting model.layers.15.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.16.input_layernorm.weight from float16 to float32\nConverting model.layers.16.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.17.input_layernorm.weight from float16 to float32\nConverting model.layers.17.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.18.input_layernorm.weight from float16 to float32\nConverting model.layers.18.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.19.input_layernorm.weight from float16 to float32\nConverting model.layers.19.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.20.input_layernorm.weight from float16 to float32\nConverting model.layers.20.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.21.input_layernorm.weight from float16 to float32\nConverting model.layers.21.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.22.input_layernorm.weight from float16 to float32\nConverting model.layers.22.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.23.input_layernorm.weight from float16 to float32\nConverting model.layers.23.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.24.input_layernorm.weight from float16 to float32\nConverting model.layers.24.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.25.input_layernorm.weight from float16 to float32\nConverting model.layers.25.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.26.input_layernorm.weight from float16 to float32\nConverting model.layers.26.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.27.input_layernorm.weight from float16 to float32\nConverting model.layers.27.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.28.input_layernorm.weight from float16 to float32\nConverting model.layers.28.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.29.input_layernorm.weight from float16 to float32\nConverting model.layers.29.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.30.input_layernorm.weight from float16 to float32\nConverting model.layers.30.post_attention_layernorm.weight from float16 to float32\nConverting model.layers.31.input_layernorm.weight from float16 to float32\nConverting model.layers.31.post_attention_layernorm.weight from float16 to float32\nConverting model.norm.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.embeddings.class_embedding from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.post_layernorm.weight from float16 to float32\nConverting model.vision_tower.vision_tower.vision_model.post_layernorm.bias from float16 to float32\nConverting model.mm_projector.0.bias from float16 to float32\nConverting model.mm_projector.2.bias from float16 to float32\nConverting lm_head.weight from float16 to float32\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# for name, param in model.named_parameters():\n#     if param.requires_grad:\n#         print(f\"{name}: {param.numel()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:09:11.834019Z","iopub.execute_input":"2025-06-05T01:09:11.834236Z","iopub.status.idle":"2025-06-05T01:09:11.837426Z","shell.execute_reply.started":"2025-06-05T01:09:11.834218Z","shell.execute_reply":"2025-06-05T01:09:11.836695Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from tqdm import tqdm\nfrom torch.nn.functional import cross_entropy\n\nclass_labels = [\"TI-RADS 2\", \"TI-RADS 3\", \"TI-RADS 4\", \"TI-RADS 5\"]\nlabel_to_index = {label: i for i, label in enumerate(class_labels)}\n\n# model.to(device)\nmodel.train()\noptimizer = torch.optim.AdamW(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=1e-5,\n    weight_decay=0.0\n)\nnum_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"🔍 Trainable parameters: {num_trainable}\")\n\nfor epoch in range(20):\n    total_loss = 0\n    for batch in tqdm(val_loader):\n        optimizer.zero_grad()\n\n        # Process image\n        processed = image_processor(images=batch['pixel_values'], return_tensors=\"pt\").to(device)\n        # pixel_values = processed[\"pixel_values\"]\n        pixel_values = processed[\"pixel_values\"]\n\n        batch_size = processed[\"pixel_values\"].size(0)\n        prompts = [\"The TI-RADS classification of the thyroid ultrasound image is:\"] * batch_size\n        inputs = tokenizer(prompts, return_tensors=\"pt\", padding='longest', truncation=True).to(device)\n        \n        outputs = model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            images=pixel_values,\n            output_hidden_states=True,\n            return_dict=True\n        )\n\n        # print(batch['labels'])\n        true_labels = batch['labels'].to(device)  # Ensure labels are 0–3\n        # print(\"Labels:\", true_labels)\n        # print(\"Min label:\", true_labels.min().item(), \"Max label:\", true_labels.max().item())\n        assert torch.all((true_labels >= 0) & (true_labels < 4)), \"🚨 Invalid label detected!\"\n\n        # with torch.no_grad():\n        #     hidden_check = outputs.hidden_states[-1][:, -1, :].cpu().numpy()\n        #     print(\"Embed mean/std:\", hidden_check.mean(), hidden_check.std())\n\n        hidden = outputs.hidden_states[-1][:, -1, :].float()\n        logits = model.classifier(hidden)  # shape: [B, 4]\n        loss = cross_entropy(logits, true_labels)\n        # print(\"Logits stats — mean:\", logits.mean().item(), \"max:\", logits.max().item())\n        # print(\"Loss:\", loss.item())\n        if not torch.isfinite(loss):\n            print(f\"🚨 Non-finite loss at step {epoch}, skipping.\")\n            continue\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:23:50.078417Z","iopub.execute_input":"2025-06-05T01:23:50.078795Z","iopub.status.idle":"2025-06-05T01:36:59.804209Z","shell.execute_reply.started":"2025-06-05T01:23:50.078767Z","shell.execute_reply":"2025-06-05T01:36:59.803374Z"}},"outputs":[{"name":"stdout","text":"🔍 Trainable parameters: 296964\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss = 146.3092\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss = 116.2920\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss = 114.2044\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss = 114.0463\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss = 114.0044\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss = 113.9740\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Loss = 113.9537\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Loss = 113.9392\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Loss = 113.9284\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss = 113.9199\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Loss = 113.9131\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Loss = 113.9079\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Loss = 113.9035\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Loss = 113.8994\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Loss = 113.8963\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Loss = 113.8932\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Loss = 113.8909\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Loss = 113.8889\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Loss = 113.8870\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 45/45 [00:39<00:00,  1.14it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Loss = 113.8854\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"dtypes = set(p.dtype for p in model.parameters())\nprint(\"Model parameter dtypes:\", dtypes)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T01:10:48.655909Z","iopub.status.idle":"2025-06-05T01:10:48.656188Z","shell.execute_reply.started":"2025-06-05T01:10:48.656053Z","shell.execute_reply":"2025-06-05T01:10:48.656064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}