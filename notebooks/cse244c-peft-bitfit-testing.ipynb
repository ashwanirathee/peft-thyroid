{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2733529,"sourceType":"datasetVersion","datasetId":1634980},{"sourceId":8669991,"sourceType":"datasetVersion","datasetId":5195844},{"sourceId":12054481,"sourceType":"datasetVersion","datasetId":7577241}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install the requirments for ENV setup","metadata":{}},{"cell_type":"code","source":"!pip install pip \n!pip install torch==2.1 \n!pip install transformers==4.34.1 \n!pip install accelerate==0.23.0 \n!pip install bitsandbytes==0.41.1 \n!pip install llava-torch==1.1.1 \n# !pip install gdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# load the lib and modules required for test run of LLMs","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:40:05.813101Z","iopub.execute_input":"2023-11-11T08:40:05.813542Z","iopub.status.idle":"2023-11-11T08:40:05.818897Z","shell.execute_reply.started":"2023-11-11T08:40:05.813506Z","shell.execute_reply":"2023-11-11T08:40:05.817802Z"}}},{"cell_type":"code","source":"import textwrap\nfrom io import BytesIO\n\nimport requests\nimport torch\nfrom llava.constants import DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\nfrom llava.conversation import SeparatorStyle, conv_templates\nfrom llava.mm_utils import (\n    KeywordsStoppingCriteria,\n    get_model_name_from_path,\n    process_images,\n    tokenizer_image_token,\n)\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:27:09.296279Z","iopub.execute_input":"2025-06-05T05:27:09.296511Z","iopub.status.idle":"2025-06-05T05:27:09.301012Z","shell.execute_reply.started":"2025-06-05T05:27:09.296490Z","shell.execute_reply":"2025-06-05T05:27:09.300184Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# disable_torch_init()\n# MODEL = \"microsoft/llava-med-v1.5-mistral-7b\"\n# model_name = get_model_name_from_path(MODEL)\n# model_name\ndisable_torch_init()\nMODEL = \"/kaggle/input/llava-med-bitfitbase-epoch5/other/default/1/llava-med-bitfitbase-train3\"\nmodel_name = get_model_name_from_path(MODEL)\nmodel_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:27:09.302055Z","iopub.execute_input":"2025-06-05T05:27:09.302284Z","iopub.status.idle":"2025-06-05T05:27:09.317904Z","shell.execute_reply.started":"2025-06-05T05:27:09.302264Z","shell.execute_reply":"2025-06-05T05:27:09.316900Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'llava-med-bitfitbase-train3'"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Load the test images for testing the model","metadata":{"execution":{"iopub.status.busy":"2023-11-11T08:40:24.801991Z","iopub.execute_input":"2023-11-11T08:40:24.802597Z","iopub.status.idle":"2023-11-11T08:40:24.806589Z","shell.execute_reply.started":"2023-11-11T08:40:24.802563Z","shell.execute_reply":"2023-11-11T08:40:24.805566Z"}}},{"cell_type":"code","source":"tokenizer, model, image_processor, context_len = load_pretrained_model(\n                                                    model_path=MODEL, \n                                                    model_base=None, \n                                                    model_name=model_name, \n                                                    load_8bit=False )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:27:09.319049Z","iopub.execute_input":"2025-06-05T05:27:09.319372Z","iopub.status.idle":"2025-06-05T05:27:40.714899Z","shell.execute_reply.started":"2025-06-05T05:27:09.319339Z","shell.execute_reply":"2025-06-05T05:27:40.713862Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\nSome weights of the model checkpoint at /kaggle/input/llava-med-bitfitbase-epoch5/other/default/1/llava-med-bitfitbase-train3 were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'classifier.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'classifier.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias']\n- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Load the saved dataset\ndata = torch.load('/kaggle/input/peft-thyroid-dataset/augmented_dataset.pt', weights_only=False)\n\n# Print sizes\nprint(\"Train images:\", data['train_images'].shape)\nprint(\"Train labels:\", len(data['train_labels']))\nprint(\"Train IDs:\", len(data['train_ids']))  # list of strings\n\nprint(\"Val images:\", data['val_images'].shape)\nprint(\"Val labels:\", len(data['val_labels']))\nprint(\"Val IDs:\", len(data['val_ids']))\n\nprint(\"Test images:\", data['test_images'].shape)\nprint(\"Test labels:\", len(data['test_labels']))\nprint(\"Test IDs:\", len(data['test_ids']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:27:48.145272Z","iopub.execute_input":"2025-06-05T05:27:48.145707Z","iopub.status.idle":"2025-06-05T05:27:53.765752Z","shell.execute_reply.started":"2025-06-05T05:27:48.145677Z","shell.execute_reply":"2025-06-05T05:27:53.764832Z"}},"outputs":[{"name":"stdout","text":"Train images: torch.Size([1035, 3, 360, 560])\nTrain labels: 1035\nTrain IDs: 1035\nVal images: (45, 3, 360, 560)\nVal labels: 45\nVal IDs: 45\nTest images: (45, 3, 360, 560)\nTest labels: 45\nTest IDs: 45\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def process_image(image):\n    args = {\"image_aspect_ratio\": \"pad\"}\n    image_tensor = process_images([image], image_processor, args)\n    return image_tensor.to(model.device, dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:27:53.767290Z","iopub.execute_input":"2025-06-05T05:27:53.767826Z","iopub.status.idle":"2025-06-05T05:27:53.771952Z","shell.execute_reply.started":"2025-06-05T05:27:53.767803Z","shell.execute_reply":"2025-06-05T05:27:53.771029Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def create_prompt(prompt: str):\n    CONV_MODE = 'llava_v0'\n    conv = conv_templates[CONV_MODE].copy()\n    roles = conv.roles\n    prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n    conv.append_message(roles[0], prompt)\n    conv.append_message(roles[1], None)\n    return conv.get_prompt(), conv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:27:53.772972Z","iopub.execute_input":"2025-06-05T05:27:53.773219Z","iopub.status.idle":"2025-06-05T05:27:53.783541Z","shell.execute_reply.started":"2025-06-05T05:27:53.773200Z","shell.execute_reply":"2025-06-05T05:27:53.782888Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    balanced_accuracy_score, matthews_corrcoef, confusion_matrix,\n    classification_report\n)\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\ndef get_report(gt_labels, predicted_labels, all_probs, report_name):\n    print(\"Report:\", report_name)\n    true_labels = gt_labels\n    pred_labels = predicted_labels\n    # Optional: sort class labels explicitly\n    classes = sorted(set(true_labels + pred_labels))  # [2, 3, 4]\n    \n    # Binarize true labels\n    y_true_bin = label_binarize(gt_labels, classes=class_labels)\n    y_prob = np.array(all_probs)\n    \n    # Compute metrics\n    roc_auc = roc_auc_score(y_true_bin, y_prob, average='macro')\n    pr_auc = average_precision_score(y_true_bin, y_prob, average='macro')\n    \n    # Compute standard multi-class metrics\n    metrics = {\n        \"Accuracy\": accuracy_score(true_labels, pred_labels),\n        \"Macro Precision\": precision_score(true_labels, pred_labels, average='macro', zero_division=0),\n        \"Macro Recall\": recall_score(true_labels, pred_labels, average='macro', zero_division=0),\n        \"Macro F1 Score\": f1_score(true_labels, pred_labels, average='macro', zero_division=0),\n        \"Weighted F1 Score\": f1_score(true_labels, pred_labels, average='weighted', zero_division=0),\n        \"Balanced Accuracy\": balanced_accuracy_score(true_labels, pred_labels),\n        \"Matthews Correlation Coefficient\": matthews_corrcoef(true_labels, pred_labels),\n        \"AUC-ROC (macro)\": roc_auc,\n        \"AUC-PR (macro\": pr_auc\n    }\n    \n    # Display metrics summary table\n    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Score'])\n    print(\"=== Multi-class Summary Metrics ===\")\n    print(metrics_df.round(4))\n    \n    # Optional: show full classification report\n    print(\"\\n=== Per-class Report ===\")\n    print(classification_report(true_labels, pred_labels, digits=4))\n    print(\"\\n========================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:27:53.785081Z","iopub.execute_input":"2025-06-05T05:27:53.785319Z","iopub.status.idle":"2025-06-05T05:27:53.800652Z","shell.execute_reply.started":"2025-06-05T05:27:53.785298Z","shell.execute_reply":"2025-06-05T05:27:53.799932Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef classify_image_by_logits(image: Image, class_labels: list[str], prompt: str):\n    image_tensor = process_image(image)\n    prompt, conv = create_prompt(prompt)\n    \n    input_ids = tokenizer_image_token(\n        prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n    ).unsqueeze(0).to(model.device)\n\n    with torch.inference_mode():\n        outputs = model(\n            input_ids=input_ids,\n            images=image_tensor,\n            return_dict=True\n        )\n\n    # Get the logits for the next token prediction (only last token in prompt)\n    next_token_logits = outputs.logits[0, -1]  # shape: [vocab_size]\n\n    # Compute logits for all class labels\n    label_scores = []\n    for label in class_labels:\n        token_ids = tokenizer(label, add_special_tokens=False)[\"input_ids\"]\n        logit_sum = sum([next_token_logits[token_id].item() for token_id in token_ids])\n        label_scores.append(logit_sum)\n\n    # Convert logits to probabilities\n    label_scores_tensor = torch.tensor(label_scores)\n    softmax_probs = F.softmax(label_scores_tensor, dim=0).tolist()\n\n    # Get predicted label\n    best_index = int(torch.argmax(label_scores_tensor))\n    predicted_label = class_labels[best_index]\n\n    return predicted_label, softmax_probs\n\ndef map_label_to_tirads(label):\n    if \"2\" in label: return \"TI-RADS 2\"\n    elif \"3\" in label: return \"TI-RADS 3\"\n    elif \"4\" in label: return \"TI-RADS 4\"\n    elif \"5\" in label: return \"TI-RADS 5\"\n    else: return \"Unknown\"\n    \ncustom_prompt = \"The TI-RADS classification of the thyroid ultrasound image is: \"\nclass_labels = [\"TI-RADS 2\", \"TI-RADS 3\", \"TI-RADS 4\", \"TI-RADS 5\"]\n\nprint(len(data['test_labels']))\ngt_labels = []\npredicted_labels = []\nall_probs = []\nfor i in range(len(data['test_labels'])):\n    img_tensor = data['test_images'][i]  # shape: (3, 360, 560)\n\n    gt_label = map_label_to_tirads(data['test_labels'][i])\n    gt_labels.append(gt_label)\n\n    pred, probs = classify_image_by_logits(img_tensor, class_labels, custom_prompt)\n    predicted_labels.append(pred)\n\n    all_probs.append(probs)\n    print(f\"PatientId: {data['test_ids'][i]}, Label:{gt_label}, Predicted: {pred}\")\n\nbaseline_report = get_report(gt_labels, predicted_labels, all_probs, \"BitFit Test Data Report\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:27:54.953497Z","iopub.execute_input":"2025-06-05T05:27:54.954178Z","iopub.status.idle":"2025-06-05T05:29:40.072849Z","shell.execute_reply.started":"2025-06-05T05:27:54.954153Z","shell.execute_reply":"2025-06-05T05:29:40.071764Z"}},"outputs":[{"name":"stdout","text":"45\nPatientId: 259, Label:TI-RADS 3, Predicted: TI-RADS 2\nPatientId: 18, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 91, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 79, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 311, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 32, Label:TI-RADS 2, Predicted: TI-RADS 3\nPatientId: 198, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 176, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 83, Label:TI-RADS 5, Predicted: TI-RADS 5\nPatientId: 346, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 204, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 20, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 129, Label:TI-RADS 3, Predicted: TI-RADS 2\nPatientId: 110, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 22, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 105, Label:TI-RADS 3, Predicted: TI-RADS 4\nPatientId: 53, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 12, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 269, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 135, Label:TI-RADS 5, Predicted: TI-RADS 3\nPatientId: 373, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 376, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 242, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 251, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 207, Label:TI-RADS 3, Predicted: TI-RADS 4\nPatientId: 203, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 248, Label:TI-RADS 2, Predicted: TI-RADS 4\nPatientId: 399, Label:TI-RADS 2, Predicted: TI-RADS 3\nPatientId: 369, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 19, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 286, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 313, Label:TI-RADS 5, Predicted: TI-RADS 4\nPatientId: 202, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 382, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 337, Label:TI-RADS 2, Predicted: TI-RADS 3\nPatientId: 81, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 359, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 108, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 303, Label:TI-RADS 2, Predicted: TI-RADS 4\nPatientId: 293, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 383, Label:TI-RADS 5, Predicted: TI-RADS 3\nPatientId: 295, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 197, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 387, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 70, Label:TI-RADS 4, Predicted: TI-RADS 3\nReport: BitFit Test Data Report\n=== Multi-class Summary Metrics ===\n                                   Score\nAccuracy                          0.3556\nMacro Precision                   0.4375\nMacro Recall                      0.1797\nMacro F1 Score                    0.2442\nWeighted F1 Score                 0.4458\nBalanced Accuracy                 0.1797\nMatthews Correlation Coefficient -0.0212\nAUC-ROC (macro)                   0.4955\nAUC-PR (macro                     0.3424\n\n=== Per-class Report ===\n              precision    recall  f1-score   support\n\n   TI-RADS 2     0.0000    0.0000    0.0000         5\n   TI-RADS 3     0.0000    0.0000    0.0000         4\n   TI-RADS 4     0.7500    0.4688    0.5769        32\n   TI-RADS 5     1.0000    0.2500    0.4000         4\n\n    accuracy                         0.3556        45\n   macro avg     0.4375    0.1797    0.2442        45\nweighted avg     0.6222    0.3556    0.4458        45\n\n\n========================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"baseline_report = get_report(gt_labels, predicted_labels, all_probs, \"BitFit Test Data Report\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_prompt = \"The TI-RADS classification of the thyroid ultrasound image is: \"\nclass_labels = [\"TI-RADS 2\", \"TI-RADS 3\", \"TI-RADS 4\", \"TI-RADS 5\"]\n\nprint(len(data['val_labels']))\ngt_labels = []\npredicted_labels = []\nall_probs = []\nfor i in range(len(data['val_labels'])):\n    img_tensor = data['val_images'][i]  # shape: (3, 360, 560)\n\n    gt_label = map_label_to_tirads(data['val_labels'][i])\n    gt_labels.append(gt_label)\n\n    pred, probs = classify_image_by_logits(img_tensor, class_labels, custom_prompt)\n    predicted_labels.append(pred)\n\n    all_probs.append(probs)\n    print(f\"PatientId: {data['val_ids'][i]}, Label:{gt_label}, Predicted: {pred}\")\nbaseline_report = get_report(gt_labels, predicted_labels, all_probs, \"BitFit Val Data Report\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T05:29:40.100082Z","iopub.execute_input":"2025-06-05T05:29:40.100349Z","iopub.status.idle":"2025-06-05T05:31:24.701209Z","shell.execute_reply.started":"2025-06-05T05:29:40.100312Z","shell.execute_reply":"2025-06-05T05:31:24.700137Z"}},"outputs":[{"name":"stdout","text":"45\nPatientId: 233, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 76, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 30, Label:TI-RADS 5, Predicted: TI-RADS 4\nPatientId: 141, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 217, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 11, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 378, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 334, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 74, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 247, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 89, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 192, Label:TI-RADS 2, Predicted: TI-RADS 4\nPatientId: 343, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 179, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 220, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 227, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 5, Label:TI-RADS 2, Predicted: TI-RADS 2\nPatientId: 335, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 372, Label:TI-RADS 5, Predicted: TI-RADS 3\nPatientId: 302, Label:TI-RADS 2, Predicted: TI-RADS 4\nPatientId: 270, Label:TI-RADS 2, Predicted: TI-RADS 3\nPatientId: 107, Label:TI-RADS 2, Predicted: TI-RADS 4\nPatientId: 320, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 122, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 374, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 128, Label:TI-RADS 5, Predicted: TI-RADS 3\nPatientId: 118, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 331, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 126, Label:TI-RADS 3, Predicted: TI-RADS 2\nPatientId: 304, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 21, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 78, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 361, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 333, Label:TI-RADS 5, Predicted: TI-RADS 4\nPatientId: 160, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 185, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 162, Label:TI-RADS 2, Predicted: TI-RADS 4\nPatientId: 284, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 201, Label:TI-RADS 2, Predicted: TI-RADS 3\nPatientId: 360, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 398, Label:TI-RADS 2, Predicted: TI-RADS 4\nPatientId: 68, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 27, Label:TI-RADS 4, Predicted: TI-RADS 3\nPatientId: 155, Label:TI-RADS 4, Predicted: TI-RADS 4\nPatientId: 94, Label:TI-RADS 5, Predicted: TI-RADS 4\nReport: BitFit Val Data Report\n=== Multi-class Summary Metrics ===\n                                   Score\nAccuracy                          0.3556\nMacro Precision                   0.2880\nMacro Recall                      0.1522\nMacro F1 Score                    0.1889\nWeighted F1 Score                 0.4183\nBalanced Accuracy                 0.1522\nMatthews Correlation Coefficient -0.0281\nAUC-ROC (macro)                   0.4941\nAUC-PR (macro                     0.3457\n\n=== Per-class Report ===\n              precision    recall  f1-score   support\n\n   TI-RADS 2     0.5000    0.1250    0.2000         8\n   TI-RADS 3     0.0000    0.0000    0.0000         1\n   TI-RADS 4     0.6522    0.4839    0.5556        31\n   TI-RADS 5     0.0000    0.0000    0.0000         5\n\n    accuracy                         0.3556        45\n   macro avg     0.2880    0.1522    0.1889        45\nweighted avg     0.5382    0.3556    0.4183        45\n\n\n========================\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}